{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e82d83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f5946ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('airbnb_train_X_price_gte_150.csv') \n",
    "y_train = pd.read_csv('airbnb_train_y_price_gte_150.csv') \n",
    "X_test = pd.read_csv('airbnb_test_X_price_gte_150.csv') \n",
    "y_test = pd.read_csv('airbnb_test_y_price_gte_150.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d288c0",
   "metadata": {},
   "outputs": [],
   "source": [
    " performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2648f241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best precision score is 0.8596689517045792\n",
      "... with parameters: {'min_samples_split': 13, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.0011, 'max_leaf_nodes': 37, 'max_depth': 46, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\davul\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "40 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\davul\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\davul\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\davul\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\davul\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.82488631 0.77844887 0.83809356 0.84800864 0.82488631 0.77844887\n",
      " 0.83055799 0.82514459 0.82627955 0.85700343 0.83807801 0.83480762\n",
      " 0.82969316 0.83886618 0.8396604  0.84422075 0.82627955 0.84811337\n",
      " 0.84925161 0.82514459 0.82901999 0.85537012 0.83987605 0.83861123\n",
      " 0.84133442 0.82627955 0.8280068  0.83581309 0.82488631 0.83796224\n",
      " 0.83965785 0.8355079  0.84592853 0.83024212 0.82514459 0.84210165\n",
      " 0.84724289 0.83834604 0.83552884 0.83929786 0.84307665 0.83057559\n",
      " 0.82627955 0.82488631 0.8389098  0.82932013 0.84913799 0.85426974\n",
      " 0.83141986 0.84673915 0.83267695 0.85717055 0.84762166 0.82488631\n",
      " 0.83581309 0.8419483  0.77844887 0.82488631 0.8317569  0.84679758\n",
      " 0.83784034 0.84437875 0.84501705 0.84708677 0.8278815  0.83118427\n",
      " 0.82488631 0.82488631 0.84455347 0.82488631 0.82488631 0.84006265\n",
      " 0.84673915 0.83247073 0.82514459 0.8360193  0.84525968 0.8323012\n",
      " 0.83834604 0.82869408 0.83581309 0.85303857 0.82488631 0.82488631\n",
      " 0.77844887 0.84422075 0.82627955 0.83581309 0.8278815  0.82488631\n",
      " 0.83878499 0.84890685 0.83304336 0.84544946 0.84276655 0.8317569\n",
      " 0.85062024 0.8280068  0.83581309 0.83793687 0.84130025 0.83923165\n",
      " 0.84212404 0.84395479 0.82488631 0.8501587  0.83923165 0.84814051\n",
      " 0.83587396 0.82488631 0.83581309 0.82488631 0.82488631 0.84491204\n",
      " 0.82514459 0.83617917 0.83581309 0.77844887 0.82514459 0.84271483\n",
      " 0.83825509 0.8278815  0.82901999 0.77844887 0.82488631 0.82488631\n",
      " 0.83803763 0.83581309 0.84395805 0.82627955 0.77844887 0.82514459\n",
      " 0.85276431 0.82627955 0.82621034 0.84756926 0.82578511 0.83581309\n",
      " 0.84221652 0.82488631 0.82970101 0.84269216 0.8278815  0.84679758\n",
      " 0.82488631 0.83609324 0.85314645 0.83145699 0.77844887 0.85146379\n",
      " 0.82059462 0.8317569  0.84276655 0.84084038 0.8325115  0.82868301\n",
      " 0.83845651 0.85081006 0.82768985 0.8429271  0.83528594 0.82514459\n",
      " 0.83143064 0.82867949 0.84216672 0.82837635 0.82514459 0.83825509\n",
      " 0.839126   0.82488631 0.83855849 0.82488631 0.84088427 0.85282387\n",
      " 0.84192335 0.82488631 0.82906729 0.84649426 0.83587396 0.8280068\n",
      " 0.8317569  0.82970101 0.82884323 0.8501587  0.84287178 0.8412033\n",
      " 0.84658679 0.83480762 0.77844887 0.83237364 0.83997506 0.84395479\n",
      " 0.77844887 0.84173701 0.84679758 0.82488631 0.83332056 0.82488631\n",
      " 0.83895825 0.83247073 0.8280068  0.83340409 0.84428996 0.82488631\n",
      " 0.84414284 0.82686914 0.83845651 0.77844887 0.82488631 0.83834604\n",
      " 0.83353865 0.82514459 0.84686788 0.83876446 0.82514459 0.85234595\n",
      " 0.83521469 0.84269216 0.82926049 0.83548537 0.84437875 0.83304336\n",
      " 0.82817806 0.84574416 0.82488631 0.85426974 0.84250571 0.82488631\n",
      " 0.84276655 0.82627955 0.84795638 0.77844887 0.83856831 0.82488631\n",
      " 0.84708677 0.8326648  0.8280068  0.82488631 0.84823064 0.82488631\n",
      " 0.84255845 0.82444721 0.82488631 0.84485417 0.83987605 0.84740936\n",
      " 0.83304336 0.83304336 0.83865213 0.84398494 0.85106965 0.83184151\n",
      " 0.85171889 0.83839466        nan 0.83924658 0.84156968 0.82488631\n",
      " 0.83572853 0.83364647 0.82488631 0.83857108 0.83764289 0.82488631\n",
      " 0.83507541 0.83898786 0.82926049 0.8389098  0.844429   0.82514459\n",
      " 0.77844887 0.82488631 0.84173701        nan 0.82797979 0.82971325\n",
      " 0.83878463 0.83189378 0.82838914 0.83922467 0.84173701 0.77844887\n",
      " 0.83548537 0.82488631 0.83195023 0.82904928 0.8280068  0.8372525\n",
      " 0.8455655  0.82488631 0.83556602 0.83878463 0.83548537 0.82488631\n",
      " 0.82627955 0.82488631 0.8278815  0.82949016 0.82488631 0.8280068\n",
      " 0.82488631 0.83595302 0.82488631 0.82667051 0.84398494 0.82514459\n",
      " 0.84523013 0.84174215 0.82488631 0.82514459 0.8278815  0.8457863\n",
      " 0.82780296 0.83026894 0.83304336 0.82743334 0.84805907 0.82488631\n",
      " 0.83304336 0.82488631 0.82904928 0.84806634 0.8127033         nan\n",
      " 0.83304336 0.82535173 0.83556602 0.82514459 0.85966895 0.77844887\n",
      " 0.82488631 0.84156968 0.84276655 0.84795638 0.82615474 0.82839603\n",
      " 0.8355079  0.83304336 0.82488631 0.84322308 0.82488631 0.8442527\n",
      " 0.82488631 0.8280068  0.82488631 0.83743648 0.82514459 0.84858524\n",
      " 0.82488631 0.8278815  0.82488631 0.83507541 0.82488631 0.8280068\n",
      " 0.84516036 0.848185   0.82488631 0.84276655 0.83581309 0.83491252\n",
      " 0.8429271  0.85151564        nan 0.84098815 0.84276655 0.8396604\n",
      " 0.84409759 0.83910646 0.84253064 0.83491274 0.8498319  0.77844887\n",
      " 0.83261222 0.8280068  0.82514459 0.82488631 0.84221652 0.84283953\n",
      " 0.83556602 0.83581309 0.83972844 0.82627955 0.85084435 0.8324992\n",
      " 0.85025907 0.82488631 0.77844887 0.8450615  0.83952217 0.82578511\n",
      " 0.82488631 0.84395479 0.82488631 0.82488631 0.82488631 0.82488631\n",
      " 0.85630879 0.8463411  0.83494371 0.83669304 0.8389098  0.84623971\n",
      " 0.82488631 0.83887417 0.84173701 0.8278815  0.83556602 0.83304336\n",
      " 0.82514459 0.82488631 0.82514459 0.82514459 0.82514459 0.83456315\n",
      " 0.82488631 0.83987605 0.84428996 0.83548537 0.83861123 0.83541542\n",
      " 0.84437711 0.83920391 0.83581309 0.83332056 0.77844887        nan\n",
      " 0.82488631 0.83886117 0.85452732 0.82488631 0.8280068  0.84494157\n",
      " 0.83895825 0.85717055        nan 0.8368684  0.83924658 0.83304336\n",
      " 0.8280068  0.85129214 0.82488631 0.82488631 0.84615998 0.85312114\n",
      " 0.82488631 0.84574416 0.8280068  0.84380148 0.83581309 0.83825509\n",
      " 0.82488631 0.84250571 0.82488631 0.83887372 0.82901999 0.83304336\n",
      " 0.8360193  0.82488631 0.83264075 0.77844887 0.83304336 0.83480762\n",
      " 0.82627955 0.82488631 0.84494157 0.84111911 0.82514459 0.82488631\n",
      " 0.83465458 0.83466557 0.82488631 0.84224584 0.83825509 0.8280068\n",
      " 0.82488631 0.83494371 0.82488631 0.82488631 0.841505   0.84271483\n",
      " 0.83527567        nan 0.8280068  0.82488631 0.84491204 0.84190699\n",
      " 0.8446265  0.83143458 0.82578511 0.82488631 0.77844887 0.8360193\n",
      " 0.84708677 0.82937615 0.82627955 0.82514459 0.84574416 0.83954753\n",
      " 0.83987605        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\davul\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.82583601 0.77774902 0.84804305 0.86585709 0.82583601 0.77774902\n",
      " 0.85636209 0.82851932 0.83082791 0.87215124 0.86008958 0.84214072\n",
      " 0.834912   0.85969102 0.84777645 0.85141261 0.83082791 0.86415235\n",
      " 0.86858114 0.82851932 0.83445672 0.86875616 0.84373893 0.84625332\n",
      " 0.84540251 0.83082791 0.83272552 0.84238417 0.82583601 0.86082043\n",
      " 0.86093392 0.85914861 0.85570267 0.85439901 0.82851932 0.87766614\n",
      " 0.86773561 0.84634707 0.84314121 0.90610739 0.85163654 0.85306695\n",
      " 0.83082791 0.82583601 0.84840174 0.86619676 0.86190135 0.87547244\n",
      " 0.85761211 0.85701515 0.84031158 0.87996307 0.88196171 0.82583601\n",
      " 0.84238417 0.86724328 0.77774902 0.82583601 0.83864725 0.85496571\n",
      " 0.86325949 0.85100697 0.85475218 0.85516646 0.83301112 0.8367705\n",
      " 0.82583601 0.82583601 0.89337242 0.82583601 0.82583601 0.84773699\n",
      " 0.85701515 0.83861476 0.82848866 0.84408099 0.85360388 0.86747184\n",
      " 0.84634707 0.83457867 0.84238417 0.87261768 0.82583601 0.82583601\n",
      " 0.77774902 0.85141261 0.83082791 0.84238417 0.83301112 0.82583601\n",
      " 0.87192333 0.87301634 0.84049152 0.85774412 0.84810619 0.83864725\n",
      " 0.85904905 0.83272552 0.84238417 0.86166173 0.86770542 0.85899528\n",
      " 0.86127303 0.85675484 0.82583601 0.86452873 0.85899528 0.87090883\n",
      " 0.8442149  0.82583601 0.84238417 0.82583601 0.82583601 0.85280786\n",
      " 0.82851932 0.84412918 0.84238417 0.77774902 0.82851932 0.86506357\n",
      " 0.84786933 0.83301112 0.83442467 0.77774902 0.82583601 0.82583601\n",
      " 0.863131   0.84238417 0.86369417 0.83082791 0.77774902 0.82851932\n",
      " 0.87267887 0.83082791 0.83068463 0.86164054 0.82845794 0.84238417\n",
      " 0.84752902 0.82583601 0.83672211 0.84997759 0.83301112 0.85496571\n",
      " 0.82583601 0.84646754 0.87294401 0.85873642 0.77774902 0.87238195\n",
      " 0.91994579 0.83864725 0.84810619 0.86727895 0.83800353 0.83272567\n",
      " 0.84714625 0.86080396 0.86064412 0.85378376 0.86091193 0.82851932\n",
      " 0.88405785 0.85364147 0.87821639 0.85363983 0.82851932 0.84786933\n",
      " 0.87677077 0.82583601 0.84879114 0.82583601 0.86165467 0.8761817\n",
      " 0.87121328 0.82583601 0.87539297 0.85480087 0.8442149  0.83272552\n",
      " 0.83864725 0.83672211 0.83422169 0.86452873 0.85547508 0.87631797\n",
      " 0.86534297 0.84214072 0.77774902 0.85721671 0.84808153 0.85661873\n",
      " 0.77774902 0.84544502 0.85496571 0.82583601 0.83957959 0.82583601\n",
      " 0.86269711 0.83861476 0.83272552 0.90010014 0.84936169 0.82583601\n",
      " 0.85141063 0.87565279 0.84714625 0.77774902 0.82583601 0.84634707\n",
      " 0.83948833 0.82851932 0.86138449 0.84853565 0.82851932 0.87188831\n",
      " 0.8589266  0.84997759 0.83397202 0.84597667 0.85100697 0.84049152\n",
      " 0.86268563 0.8526681  0.82583601 0.87571836 0.84772977 0.82583601\n",
      " 0.84810619 0.83082791 0.8637977  0.77774902 0.8437397  0.82583601\n",
      " 0.85513746 0.85844363 0.83272552 0.82583601 0.86356014 0.82583601\n",
      " 0.87178327 0.86028108 0.82583601 0.86537142 0.84373893 0.85400728\n",
      " 0.84049152 0.84049152 0.86285301 0.85713571 0.87281482 0.86168746\n",
      " 0.86481235 0.84167561        nan 0.86253741 0.84903845 0.82583601\n",
      " 0.84496043 0.83978321 0.82583601 0.86759209 0.86650993 0.82583601\n",
      " 0.86234046 0.85634141 0.83452251 0.84840174 0.85691878 0.82851932\n",
      " 0.77774902 0.82583601 0.84544502        nan 0.84811079 0.83512085\n",
      " 0.8638217  0.88994859 0.86531449 0.86846417 0.84544502 0.77774902\n",
      " 0.84597667 0.82583601 0.86691622 0.83272057 0.83272552 0.87319222\n",
      " 0.87879264 0.82583601 0.8428255  0.8638217  0.84597667 0.82583601\n",
      " 0.83082791 0.82583601 0.83301112 0.85277451 0.82583601 0.83272552\n",
      " 0.82583601 0.86035799 0.82583601 0.90642108 0.85716222 0.82851932\n",
      " 0.85216545 0.849096   0.82583601 0.82851932 0.83301112 0.85623134\n",
      " 0.85716496 0.8568388  0.84049152 0.86958734 0.85843965 0.82583601\n",
      " 0.84049152 0.82583601 0.83272057 0.8722669  0.92290471        nan\n",
      " 0.84049152 0.83083726 0.8428255  0.82851932 0.89075803 0.77774902\n",
      " 0.82583601 0.84903845 0.84810619 0.86363532 0.83070565 0.88695326\n",
      " 0.85914861 0.84049152 0.82583601 0.85166463 0.82583601 0.86838978\n",
      " 0.82583601 0.83272552 0.82583601 0.89230488 0.82851932 0.8708747\n",
      " 0.82583601 0.83301112 0.82583601 0.86234046 0.82583601 0.83272552\n",
      " 0.85477917 0.88034304 0.82583601 0.84810619 0.84238417 0.86760625\n",
      " 0.85378376 0.87016589        nan 0.89981119 0.84810619 0.84777645\n",
      " 0.851488   0.89307503 0.84850985 0.84409992 0.86895721 0.77774902\n",
      " 0.85937176 0.83272552 0.82851932 0.82583601 0.84752902 0.8644927\n",
      " 0.8428255  0.84238417 0.86670275 0.83082791 0.865048   0.84349483\n",
      " 0.88727659 0.82583601 0.77774902 0.8598196  0.84630491 0.82845794\n",
      " 0.82583601 0.85675484 0.82583601 0.82583601 0.82583601 0.82583601\n",
      " 0.87830988 0.86593945 0.84140665 0.84554907 0.84840174 0.85271306\n",
      " 0.82583601 0.8436804  0.84544502 0.83301112 0.8428255  0.84049152\n",
      " 0.82851932 0.82583601 0.82851932 0.82848866 0.82848866 0.85931484\n",
      " 0.82583601 0.84373893 0.84936169 0.84597667 0.84625332 0.86065132\n",
      " 0.86523512 0.86316983 0.84238417 0.83993316 0.77774902        nan\n",
      " 0.82583601 0.86353019 0.87946448 0.82583601 0.83272552 0.85313375\n",
      " 0.86269711 0.87996307        nan 0.86950717 0.86253741 0.84049152\n",
      " 0.83272552 0.87312399 0.82583601 0.82583601 0.87987933 0.87039344\n",
      " 0.82583601 0.8526681  0.83272552 0.86976967 0.84238417 0.84786933\n",
      " 0.82583601 0.84772977 0.82583601 0.86971414 0.83445672 0.84049152\n",
      " 0.84408099 0.82583601 0.83992283 0.77774902 0.84049152 0.84214072\n",
      " 0.83082791 0.82583601 0.85313375 0.86162006 0.82851932 0.82583601\n",
      " 0.87001736 0.84009892 0.82583601 0.85416438 0.84786933 0.83272552\n",
      " 0.82583601 0.84140665 0.82583601 0.82583601 0.86768258 0.86506357\n",
      " 0.84473815        nan 0.83272552 0.82583601 0.85280786 0.86631783\n",
      " 0.85328039 0.87748359 0.82845794 0.82583601 0.77774902 0.84408099\n",
      " 0.85516646 0.83455062 0.83082791 0.82851932 0.8526681  0.85223784\n",
      " 0.84373893        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Randomized search\n",
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,70),  \n",
    "    'min_samples_leaf': np.arange(1,30),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 200), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "006d1ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8350515 Precision=0.8528827 Recall=0.8079096 F1=0.8297872\n"
     ]
    }
   ],
   "source": [
    "\n",
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68e4da06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3024 candidates, totalling 15120 fits\n",
      "The best precision score is 0.8470867681903685\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 15, 'max_leaf_nodes': 162, 'min_impurity_decrease': 0.0048, 'min_samples_leaf': 19, 'min_samples_split': 49}\n"
     ]
    }
   ],
   "source": [
    "#Grid search\n",
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(49,52),  \n",
    "    'min_samples_leaf': np.arange(17,21),\n",
    "    'min_impurity_decrease': np.arange(0.0048, 0.0054, 0.0001),\n",
    "    'max_leaf_nodes': np.arange(162,168), \n",
    "    'max_depth': np.arange(15,21), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7221e42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8462980 Precision=0.8379374 Recall=0.8568738 F1=0.8472998\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "495bc9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm polynomial\n",
    "svm_poly_model = SVC(kernel=\"poly\", degree=5, coef0=1, C=10,probability=True)\n",
    "_ = svm_poly_model.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d58d96d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8256795 Precision=0.8130672 Recall=0.8436911 F1=0.8280961\n"
     ]
    }
   ],
   "source": [
    "model_preds = svm_poly_model.predict(X_test)\n",
    "c_matrix = confusion_matrix(y_test, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7e8336",
   "metadata": {},
   "source": [
    "It appears that the decision tree model with a grid search has precision of 0.84 where as randomized search has a precision of 0.85 and svm poly kernal model has a precision of 0.81 so when compared decision tree model is better performing among the models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
